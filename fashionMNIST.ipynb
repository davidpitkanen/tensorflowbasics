{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61619304-5cef-46b5-8e4a-1badace0aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection as train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45d7a7e2-4ccc-4661-bd84-09974400fc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "train_data, info = tfds.load('fashion_mnist', split='train', with_info=True)\n",
    "test_data, info = tfds.load('fashion_mnist', split='test', with_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21a0275e-fcfd-465f-b41e-ec5a1d2aaba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps 1. reshape to flatten (data is initially 28 * 28), cast from integer to float, normalize to a value between 0 and 1 then return tuple\n",
    "def format_image(data):\n",
    "    image = data['image']\n",
    "    image = tf.reshape(image, [-1])\n",
    "    image = tf.cast(image, 'float32')\n",
    "    image = image / 255.0\n",
    "    return image, data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba12e898-a039-4ce8-94b7-d1aee1a446f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function format_image at 0x00000152DCAC6CA0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function format_image at 0x00000152DCAC6CA0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function format_image at 0x00000152DCAC6CA0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.map(format_image)\n",
    "test_data = test_data.map(format_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37c8387a-a687-416f-891d-dfe0bf6b0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train = train_data.shuffle(buffer_size=1024).batch(batch_size)\n",
    "test = test_data.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e17b62-1bdb-4406-a602-4cec03c9049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_metric = tf.keras.metrics.SparseCategoricalA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7bdf504-9a52-40cc-bdd6-9f0cd2d6e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    inputs = tf.keras.Input(shape=(784,), name='digits')\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "    outputs = tf.keras.layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b2eabe1-9170-473b-aa44-07285a2da2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfb601b7-23c1-4b16-a5d3-04946b145b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gradient(optimizer, model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = loss_object(y_true=y, y_pred=logits)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    \n",
    "    return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3352617e-f523-4347-b0a8-6dc51e0d4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_for_one_epoch():\n",
    "    losses = []\n",
    "    pbar = tqdm(total=len(list(enumerate(train))), position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}')\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train):\n",
    "        logits, loss_value = apply_gradient(optimizer, model, x_batch_train, y_batch_train)\n",
    "        losses.append(loss_value)\n",
    "        pbar.set_description(\"training for step %s: %.4f\", (int(step), float(loss_value)))\n",
    "        pbar.update()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a32c9d56-bfe3-413b-a997-39f04750530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_validation():\n",
    "    losses = []\n",
    "    for x_val, y_val in test:\n",
    "        val_logits = model(x_val)\n",
    "        val_loss = loss_object(y_true=y_val, y_pred=val_logits)\n",
    "        losses.append(val_loss)\n",
    "        val_acc_metric(y_val, val_logits)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02e60017-e697-4699-aebc-d7f0b2de6c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of epoch %d (0,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for step %s: %.4f: 100%|█████████████████████████████████████████████████████████████████████████████| 938/938\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_acc_metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart of epoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, (epoch,))\n\u001b[0;32m      8\u001b[0m losses_train \u001b[38;5;241m=\u001b[39m train_data_for_one_epoch()\n\u001b[1;32m----> 9\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mval_acc_metric\u001b[49m\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m     11\u001b[0m losses_val \u001b[38;5;241m=\u001b[39m perform_validation()\n\u001b[0;32m     12\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m val_acc_metric\u001b[38;5;241m.\u001b[39mresult()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_acc_metric' is not defined"
     ]
    }
   ],
   "source": [
    "model = base_model()\n",
    "\n",
    "epochs = 10\n",
    "epochs_val_losses, epochs_train_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    print(\"start of epoch %d\", (epoch,))\n",
    "    \n",
    "    losses_train = train_data_for_one_epoch()\n",
    "    train_acc = val_acc_metric.result()\n",
    "    \n",
    "    losses_val = perform_validation()\n",
    "    val_acc = val_acc_metric.result()\n",
    "    \n",
    "    losses_train_mean = np.mean(losses_train)\n",
    "    losses_val_mean = np.mean(losses_val)\n",
    "    epochs_val_losses = epochs_val_losses.append(losses_val_mean)\n",
    "    epochs_train_losses = epochs_train_losses.append(losses_train_mean)\n",
    "    \n",
    "    train_acc_metric.reset_states()\n",
    "    val_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e345e0-6960-47cb-9694-1fd6b3a5cfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677df12-790a-444c-a562-e573eb0b899f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
